{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab 4 - Model Training with AutomatedML\n",
    "\n",
    "In this lab you will use the automated machine learning (*AutomatedML*) capabilities within the Azure Machine Learning service.\n",
    "\n",
    "Automated machine learning picks an algorithm and hyperparameters for you and generates a model ready for deployment. \n",
    "\n",
    "\n",
    "\n",
    "![AutomatedML](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/automated-machine-learning.png)\n",
    "\n",
    "\n",
    "\n",
    "We will continue with the same scenario as in  Lab 1 and Lab 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK Version: 1.0.23\n"
     ]
    }
   ],
   "source": [
    "# Verify AML SDK Installed\n",
    "# view version history at https://pypi.org/project/azureml-sdk/#history \n",
    "import azureml.core\n",
    "print(\"SDK Version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: C:\\Users\\byteb\\Desktop\\MTC_AzureAILabs\\ML-AML-Walkthrough\\.azureml\\config.json\n",
      "MLOps\n",
      "DSIMLOps\n",
      "westeurope\n",
      "d341319a-6269-4e4c-8fbe-3a89c8eb145b\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "# Read the workspace config from file\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model using AutomatedML\n",
    "\n",
    "\n",
    "To train a model using AutoML you need only provide a configuration for AutoML that defines items such as the type of model (classification or regression), the performance metric to optimize, exit criteria in terms of max training time and iterations and desired performance, any algorithms that should not be used, and the path into which to output the results. This configuration is specified using the AutomMLConfig class, which is then used to drive the submission of an experiment via experiment.submit. When AutoML finishes the parent run, you can easily get the best performing run and model from the returned run object by using run.get_output().\n",
    "\n",
    "### Create/Get Azure ML Compute cluster\n",
    "\n",
    "We are reusing the cluster created in Lab 1. In case you removed the cluster the below code snippet is going to re-create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target, using this compute target instead of creating:  cpu-cluster\n"
     ]
    }
   ],
   "source": [
    "# Create an Azure ML Compute cluster\n",
    "\n",
    "# Create Azure ML cluster\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"cpu-cluster\"\n",
    "cluster_min_nodes = 1\n",
    "cluster_max_nodes = 3\n",
    "vm_size = \"STANDARD_DS11_V2\"\n",
    "\n",
    "# Check if the cluster exists. If yes connect to it\n",
    "if cluster_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[cluster_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('Found existing compute target, using this compute target instead of creating:  ' + cluster_name)\n",
    "    else:\n",
    "        print(\"Error: A compute target with name \",cluster_name,\" was found, but it is not of type AmlCompute.\")\n",
    "else:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size, \n",
    "                                                                min_nodes = cluster_min_nodes, \n",
    "                                                                max_nodes = cluster_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current BatchAI cluster status, use the 'status' property    \n",
    "    print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Get Data script\n",
    "\n",
    "If you are using a remote compute to run your Automated ML experiments - which is our scenario, the data fetch must be wrapped in a separate python script that implements get_data() function. This script is run on the remote compute where the automated ML experiment is run. get_data() eliminates the need to fetch the data over the wire for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "project_folder = './project'\n",
    "script_name = 'get_data.py'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./project/get_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $project_folder/get_data.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def get_data():\n",
    " \n",
    "    # Load the dataset\n",
    "    data_folder = os.environ[\"AZUREML_DATAREFERENCE_workspaceblobstore\"]\n",
    "    file_name = os.path.join(data_folder, 'banking_train.csv')\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "    # Preprocess the data\n",
    "    feature_columns = [\n",
    "                   # Demographic\n",
    "                   'age', \n",
    "                   'job', \n",
    "                   'education', \n",
    "                   'marital',  \n",
    "                   'housing', \n",
    "                   'loan', \n",
    "                   # Previous campaigns\n",
    "                   'month',\n",
    "                   'campaign',\n",
    "                   'poutcome',\n",
    "                   # Economic indicators\n",
    "                   'emp_var_rate',\n",
    "                   'cons_price_idx',\n",
    "                   'cons_conf_idx',\n",
    "                   'euribor3m',\n",
    "                   'nr_employed']\n",
    "\n",
    "    df = df[feature_columns + ['y']]\n",
    "    features = df.drop(['y'], axis=1)                                         \n",
    "    \n",
    "    # Flatten labes\n",
    "    labels = np.ravel(df.y)    \n",
    "    \n",
    "    return { \"X\" : features, \"y\" : labels}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure datastore and data reference\n",
    "\n",
    "The training files have been uploaded to the workspace's default datastore during the previous labs. We will configure AutomatedML to automatically download the files onto the nodes of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default datastore for training data: \n",
      "workspaceblobstore AzureBlob mlops9832575762 azureml-blobstore-99d54524-91d7-4821-9e99-a3309fe1bc59\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Datastore\n",
    "from azureml.core.runconfig import DataReferenceConfiguration\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "print(\"Using the default datastore for training data: \")\n",
    "print(ds.name, ds.datastore_type, ds.account_name, ds.container_name)\n",
    "\n",
    "dr = DataReferenceConfiguration(datastore_name=ds.name, \n",
    "                   path_on_datastore='datasets', \n",
    "                   path_on_compute='datasets',\n",
    "                   mode='download', # download files from datastore to compute target\n",
    "                   overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Docker run configuration\n",
    "We will run Automated ML jobs in a custom docker image that will include dependencies required by get_data() script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core import Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "# create a new RunConfig object\n",
    "run_config = RunConfiguration(framework=\"python\")\n",
    "\n",
    "# Azure ML Compute cluster for Automated ML jobs require docker.\n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "# Set compute target to Azure ML Compute cluster\n",
    "run_config.target = compute_target\n",
    "\n",
    "# Set data references\n",
    "run_config.data_references = {ds.name: dr}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Configure Automated ML run.\n",
    "\n",
    "Automated ML runs can be controlled using a number of configuration parameters. \n",
    "\n",
    "\n",
    "For our run we will use the following configuration:\n",
    "- Train a classification task\n",
    "- Execute at most 25 iterations\n",
    "- Use *normalized macro recall* as a primary performance metrics\n",
    "- Use 5-fold cross validation for model evaluation\n",
    "- Run the iterations on 3 nodes of a cluster\n",
    "- Use 1 core per iteration\n",
    "- Automatically pre-process data\n",
    "- Exit if the primary metrics is higher than 0.9\n",
    "- Limit the model selection to *SVM*, *LogisticRegression*, *LightGBM*, *TensorFlowDNN* and *RandomForest* models\n",
    "\n",
    "We configured the last setting to demonstrate *white listing* capabilities of *AutomatedML*. Unless you have a strong basis for excluding or choosing certain models you are usually better of leaving the decision to *AutomatedML* - assuming that you have enough time and resources for running through many (more than 100) iterations.\n",
    "\n",
    "\n",
    "We have configured our run to automatically pre-process data.\n",
    "\n",
    "As a result, the following data preprocessing steps are performed automatically:\n",
    "1.\tDrop high cardinality or no variance features\n",
    "    * Drop features with no useful information from training and validation sets. These include features with all values missing, same value across all rows or with extremely high cardinality (e.g., hashes, IDs or GUIDs).\n",
    "1.\tMissing value imputation\n",
    "    *\tFor numerical features, impute missing values with average of values in the column.\n",
    "    *\tFor categorical features, impute missing values with most frequent value.\n",
    "1.\tGenerate additional features\n",
    "    * For DateTime features: Year, Month, Day, Day of week, Day of year, Quarter, Week of the year, Hour, Minute, Second.\n",
    "    * For Text features: Term frequency based on word unigram, bi-grams, and tri-gram, Count vectorizer.\n",
    "1.\tTransformations and encodings\n",
    "    * Numeric features with very few unique values transformed into categorical features.\n",
    "    * Depending on cardinality of categorical features, perform label encoding or (hashing) one-hot encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.train.automl.run import AutoMLRun\n",
    "import logging\n",
    "\n",
    "\n",
    "automl_config = AutoMLConfig(run_configuration = run_config,\n",
    "                             task = 'classification',\n",
    "                             debug_log = 'automl_errors.log',\n",
    "                             primary_metric = 'norm_macro_recall',\n",
    "                             iterations = 25,\n",
    "                             n_cross_validations = 5,\n",
    "                             max_concurrent_iterations = cluster_max_nodes,\n",
    "                             max_cores_per_iteration = 1,\n",
    "                             preprocess = True,\n",
    "                             experiment_exit_score = 0.99,\n",
    "                             #blacklist_models = ['KNN','MultinomialNaiveBayes', 'BernoulliNaiveBayes'],\n",
    "                             whitelist_models = ['LogisticRegression', 'RandomForest', 'LightGBM', 'SVM', 'TensorFlowDNN'],\n",
    "                             verbosity = logging.INFO,\n",
    "                             path = project_folder,\n",
    "                             data_script = os.path.join(project_folder, script_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run AutomatedML job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = \"propensity_to_buy_classifier_automatedml\"\n",
    "exp = Experiment(ws, experiment_name)\n",
    "tags = {\"Desc\": \"automated ml\"}\n",
    "run = exp.submit(config=automl_config, tags=tags)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call to experiment returns `AutoMLRun` object that can be used to track the run.\n",
    "\n",
    "Since the call is asynchronous, it reports a **Preparing** or **Running** state as soon as the job is started.\n",
    "\n",
    "Here is what's happening while you wait:\n",
    "\n",
    "- **Image creation**: A Docker image is created matching the Python environment specified by the RunConfiguration. The image is uploaded to the workspace. This happens only once for each Python environment since the container is cached for subsequent runs.  During image creation, logs are streamed to the run history. You can monitor the image creation progress using these logs.\n",
    "\n",
    "- **Scaling**: If the remote cluster requires more nodes to execute the run than currently available, additional nodes are added automatically. \n",
    "\n",
    "- **Running**: In this stage, the Automated ML takes over and starts running experiments\n",
    "\n",
    "\n",
    "\n",
    "You can check the progress of a running job in multiple ways: Azure Portal, AML Widgets or streaming logs.\n",
    "\n",
    "### Monitor the run.\n",
    "\n",
    "We will use AML Widget to monitor the run. The widget will first report a \"loading\" status while running the first iteration. After completing the first iteration, an auto-updating graph and table will be shown. The widget will refresh once per minute, so you should see the graph update as child runs complete.\n",
    "\n",
    "The widget is asynchronous - it does not block the notebook. You can execute other cells while the widget is running.\n",
    "\n",
    "**Note:** The widget displays a link at the bottom. Use this link to open a web interface to explore the individual run details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancelling Runs\n",
    "\n",
    "You can cancel ongoing remote runs using the `cancel` and `cancel_iteration` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel the ongoing experiment and stop scheduling new iterations.\n",
    "# run.cancel()\n",
    "\n",
    "# Cancel iteration 1 and move onto iteration 2.\n",
    "# run.cancel_iteration(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the run\n",
    "\n",
    "You can  use SDK methods to fetch all the child runs and see individual metrics that we log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "children = list(run.get_children())\n",
    "metricslist = {}\n",
    "for child in children:\n",
    "    properties = child.get_properties()\n",
    "    metrics = {k: v for k, v in child.get_metrics().items() if isinstance(v, float)}\n",
    "    metricslist[int(properties['iteration'])] = metrics\n",
    "\n",
    "rundata = pd.DataFrame(metricslist).sort_index(1)\n",
    "rundata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waiting until the run finishes\n",
    "\n",
    "`wait_for_complettion` method will block till the run finishes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the run finishes.\n",
    "# run.wait_for_completion(show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the results\n",
    "\n",
    "### Retrieve the best model\n",
    "\n",
    "Below we select the best pipeline from our iterations. The get_output method returns the best run and the fitted model. The Model includes the pipeline and any pre-processing. Overloads on get_output allow you to retrieve the best run and fitted model for any logged metric or for a particular iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = run.get_output()\n",
    "print(best_run)\n",
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best model on any other metric\n",
    "\n",
    "Show the run and the model which has the smallest log_loss value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_metric = \"AUC_weighted\"\n",
    "specific_run, specific_model = run.get_output(metric = lookup_metric)\n",
    "print(specific_run)\n",
    "print(specific_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model from a Specific Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 3\n",
    "third_run, third_model = run.get_output(iteration=iteration)\n",
    "print(third_run)\n",
    "print(third_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "Load the test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder = '../datasets'\n",
    "filename = 'banking_test.csv'\n",
    "pathname = os.path.join(folder, filename)\n",
    "df = pd.read_csv(pathname, delimiter=',')\n",
    "\n",
    "\n",
    "feature_columns = [\n",
    "                   # Demographic\n",
    "                   'age', \n",
    "                   'job', \n",
    "                   'education', \n",
    "                   'marital',  \n",
    "                   'housing', \n",
    "                   'loan', \n",
    "                   # Previous campaigns\n",
    "                   'month',\n",
    "                   'campaign',\n",
    "                   'poutcome',\n",
    "                   # Economic indicators\n",
    "                   'emp_var_rate',\n",
    "                   'cons_price_idx',\n",
    "                   'cons_conf_idx',\n",
    "                   'euribor3m',\n",
    "                   'nr_employed']\n",
    "\n",
    "df_test = df[feature_columns + ['y']]\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "y_pred = fitted_model.predict(df_test.values)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(df_test.y, y_pred))\n",
    "print(\"Recall: \", recall_score(df_test.y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the best performing model for later use and deployment\n",
    "\n",
    "The best model can now be registered into *Model Registry*. \n",
    "\n",
    "If neither `metric` nor `iteration` are specified in the `register_model` call, the iteration with the best primary metric is registered.\n",
    "\n",
    "You can annotate the model with arbitrary tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the use of the root run (not best_run) to register the best model\n",
    "tags = {\"Department\": \"Marketing\"}\n",
    "model = run.register_model(description='AutoML trained propensity to buy classifier',\n",
    "                          tags=tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
